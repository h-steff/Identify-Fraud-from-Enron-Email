{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Identify Fraud from Enron Email\n",
    "\n",
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Storage\\Data\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib\n",
    "sys.path.append(\"../tools/\")\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import test_classifier\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi', 'salary_log', 'long_term_incentive_log', 'total_payments_log', 'shared_receipt_with_poi',\n",
    "                'to_poi_ratio', 'from_poi_ratio', 'messages_total', 'to_message_ratio', \n",
    "                 'from_message_ratio' ] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "    \n",
    "# Store all names (keys of dictionary) in a list\n",
    "name_list = []\n",
    "for key in data_dict:\n",
    "    #print key\n",
    "    name_list.append(key)\n",
    "    \n",
    "#print name_list\n",
    "#print len(name_list) # 146 names in this dataset\n",
    "\n",
    "#print data_dict['METTS MARK']\n",
    "\n",
    "# Store all available features in a list\n",
    "all_features = []\n",
    "\n",
    "for k in data_dict['METTS MARK']:\n",
    "    all_features.append(k)\n",
    "\n",
    "#print all_features\n",
    "\n",
    "#print len(all_features) # 21 features in total in the beginning\n",
    "#print len(data_dict['METTS MARK'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HANNON KEVIN P', 'COLWELL WESLEY', 'RIEKER PAULA H', 'KOPPER MICHAEL J', 'SHELBY REX', 'DELAINEY DAVID W', 'LAY KENNETH L', 'BOWEN JR RAYMOND M', 'BELDEN TIMOTHY N', 'FASTOW ANDREW S', 'CALGER CHRISTOPHER F', 'RICE KENNETH D', 'SKILLING JEFFREY K', 'YEAGER F SCOTT', 'HIRKO JOSEPH', 'KOENIG MARK E', 'CAUSEY RICHARD A', 'GLISAN JR BEN F']\n",
      "POIs:  18\n",
      "Total number:  146\n"
     ]
    }
   ],
   "source": [
    "# Get the number of POIs in the dataset and their names\n",
    "\n",
    "poi_list = []\n",
    "\n",
    "for i in data_dict:\n",
    "    if data_dict[i]['poi'] == 1:\n",
    "        poi_list.append(i)\n",
    "\n",
    "print poi_list # names of the POIs\n",
    "print \"POIs: \", len(poi_list) # 18 POIs in the dataset\n",
    "print \"Total number: \", len(data_dict) # Total number of people in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "no_people = 0\n",
    "\n",
    "for i in data_dict:\n",
    "    #print i\n",
    "    no_people += 1\n",
    "\n",
    "#print no_people\n",
    "\n",
    "# 146 keys in the data_dict -> 2 outliers: Total and the travel agency in the park -> remove\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outlier entries \"TOTAL\" and \"THE TRAVEL AGENCY IN THE PARK\"\n",
    "\n",
    "data_dict_clean = data_dict\n",
    "\n",
    "outlier_1 = data_dict_clean.pop(\"TOTAL\")\n",
    "outlier_2 = data_dict_clean.pop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "\n",
    "people_clean = 0\n",
    "\n",
    "for a in data_dict_clean:\n",
    "    #print a\n",
    "    people_clean += 1\n",
    "\n",
    "#print people_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outliers:  ['LOWRY CHARLES P', 'CHAN RONNIE', 'WODRASKA JOHN', 'URQUHART JOHN A', 'WHALEY DAVID A', 'MENDELSOHN JOHN', 'CLINE KENNETH W', 'WAKEHAM JOHN', 'WROBEL BRUCE', 'MEYER JEROME J', 'SCRIMSHAW MATTHEW', 'GATHMANN WILLIAM D', 'GILLIS JOHN', 'LOCKHART EUGENE E', 'PEREIRA PAULO V. FERRAZ', 'BLAKE JR. NORMAN P', 'CHRISTODOULOU DIOMEDES', 'WINOKUR JR. HERBERT S', 'YEAP SOON', 'FUGH JOHN L', 'SAVAGE FRANK', 'GRAMM WENDY L']\n",
      "22\n",
      "POIs in outliers:  []\n"
     ]
    }
   ],
   "source": [
    "# identify entries where more than a certain percentage of features has value \"NaN\"\n",
    "\n",
    "my_dataset_clean = data_dict_clean\n",
    "\n",
    "total_features = []\n",
    "\n",
    "for m in my_dataset_clean['METTS MARK']:\n",
    "    total_features.append(m)\n",
    "\n",
    "cutoff = 0.75\n",
    "\n",
    "outlier_list = []\n",
    "    \n",
    "    \n",
    "for person in my_dataset_clean:\n",
    "    nan_features = []\n",
    "    for feat in my_dataset_clean[person]:\n",
    "        if my_dataset_clean[person][feat] == \"NaN\" or my_dataset_clean[person][feat] == np.nan:\n",
    "            nan_features.append(feat)\n",
    "    #print nan_features\n",
    "    nan_ratio = float(len(nan_features))/float(len(total_features))\n",
    "    #print nan_ratio\n",
    "    if nan_ratio >= cutoff:\n",
    "        outlier_list.append(person)\n",
    "    \n",
    "    \n",
    "#print total_features\n",
    "#print len(total_features)\n",
    "print \"\\nOutliers: \", outlier_list\n",
    "print len(outlier_list)\n",
    "\n",
    "poi_list_2 = []\n",
    "\n",
    "for out in outlier_list:\n",
    "    if my_dataset_clean[out]['poi'] == 1:\n",
    "        poi_list_2.append(out)\n",
    "\n",
    "print \"POIs in outliers: \", poi_list_2\n",
    "\n",
    "# at a cutoff of 0.75 there are 22 people where >= 75% of features have a value of \"NaN\", none of those 22 people is a\n",
    "# known POI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove entries where more than a certain percentage of features has value \"NaN\"\n",
    "\n",
    "\n",
    "def remove_outliers(data, outliers):\n",
    "    data_out = data\n",
    "    for x in outliers:\n",
    "        if x in data_out:\n",
    "            data_out.pop(x)\n",
    "    return data_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = remove_outliers(my_dataset_clean, outlier_list)\n",
    "\n",
    "#print cleaned_data\n",
    "#print len(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features in dataset:  24\n"
     ]
    }
   ],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "\n",
    "# function for transforming feature into log10\n",
    "\n",
    "my_dataset = cleaned_data\n",
    "feat_list = ['salary', 'total_payments', 'long_term_incentive']\n",
    "\n",
    "def feature_log10(dataset, features):\n",
    "    dataset_log = dataset\n",
    "    \n",
    "    features_log = features\n",
    "    \n",
    "    for feature in features_log:\n",
    "    \n",
    "        for key in dataset_log:\n",
    "        #print key\n",
    "            if my_dataset[key][feature] != 'NaN':\n",
    "                feature_log = math.log10(float(my_dataset[key][feature]))\n",
    "                dataset_log[key][str(feature)+'_log'] = feature_log\n",
    "            else:\n",
    "                dataset_log[key][str(feature)+'_log'] = 'NaN'\n",
    "    return dataset_log\n",
    "            \n",
    "dataset_log10 = feature_log10(my_dataset, feat_list)\n",
    "\n",
    "#print dataset_log10['METTS MARK']\n",
    "print \"No. of features in dataset: \", len(dataset_log10['METTS MARK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features in dataset:  27\n"
     ]
    }
   ],
   "source": [
    "# new feature: ratio of from_messages / to_messages from total messages\n",
    "\n",
    "my_dataset_v1 = dataset_log10\n",
    "\n",
    "for key in my_dataset_v1:\n",
    "    #print key\n",
    "    if my_dataset_v1[key]['to_messages'] != 'NaN' and my_dataset_v1[key]['from_messages'] != 'NaN':\n",
    "        messages_total = float(my_dataset_v1[key]['to_messages'] + my_dataset_v1[key]['from_messages'])\n",
    "        to_message_ratio = my_dataset_v1[key]['to_messages'] / messages_total\n",
    "        from_message_ratio = my_dataset_v1[key]['from_messages'] / messages_total\n",
    "        \n",
    "        my_dataset_v1[key]['messages_total'] = messages_total\n",
    "        my_dataset_v1[key]['to_message_ratio'] = to_message_ratio\n",
    "        my_dataset_v1[key]['from_message_ratio'] = from_message_ratio\n",
    "    \n",
    "    else: \n",
    "        my_dataset_v1[key]['messages_total'] = 'NaN'\n",
    "        my_dataset_v1[key]['to_message_ratio'] = 'NaN'\n",
    "        my_dataset_v1[key]['from_message_ratio'] = 'NaN'\n",
    "\n",
    "#print my_dataset_v1['METTS MARK']\n",
    "print \"No. of features in dataset: \", len(my_dataset_v1['METTS MARK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features in dataset:  29\n"
     ]
    }
   ],
   "source": [
    "# new feature: ratio of messages from/to POI in to/from messages\n",
    "\n",
    "my_dataset_v2 = my_dataset_v1\n",
    "\n",
    "for key in my_dataset_v2:\n",
    "    #print key\n",
    "    if my_dataset_v2[key]['to_messages'] != 'NaN' and my_dataset_v2[key]['from_messages'] != 'NaN':\n",
    "        if my_dataset_v2[key]['from_poi_to_this_person'] != 'NaN' and my_dataset_v2[key]['from_this_person_to_poi'] != 'NaN':\n",
    "            if my_dataset_v2[key]['to_messages'] != 0 and my_dataset_v2[key]['from_messages'] != 0:\n",
    "                to_poi_ratio = my_dataset_v2[key]['from_poi_to_this_person'] / float(my_dataset_v2[key]['to_messages'])\n",
    "                from_poi_ratio = my_dataset_v2[key]['from_this_person_to_poi'] / float(my_dataset_v2[key]['from_messages'])\n",
    "\n",
    "                \n",
    "                my_dataset_v2[key]['to_poi_ratio'] = to_poi_ratio\n",
    "                my_dataset_v2[key]['from_poi_ratio'] = from_poi_ratio\n",
    "            else:\n",
    "                my_dataset_v2[key]['to_poi_ratio'] = 'NaN'\n",
    "                my_dataset_v2[key]['from_poi_ratio'] = 'NaN'\n",
    "                \n",
    "        else:\n",
    "                my_dataset_v2[key]['to_poi_ratio'] = 'NaN'\n",
    "                my_dataset_v2[key]['from_poi_ratio'] = 'NaN'\n",
    "    else:\n",
    "                my_dataset_v2[key]['to_poi_ratio'] = 'NaN'\n",
    "                my_dataset_v2[key]['from_poi_ratio'] = 'NaN'\n",
    "\n",
    "#print my_dataset_v2['METTS MARK']\n",
    "print \"No. of features in dataset: \", len(my_dataset_v2['METTS MARK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features in dataset:  30\n"
     ]
    }
   ],
   "source": [
    "# new feature: long_term_incentive / total_payments -> incentive_total_ratio\n",
    "\n",
    "my_dataset_v3 = my_dataset_v2\n",
    "\n",
    "\n",
    "for key in my_dataset_v3:\n",
    "    #print key\n",
    "    #print \"incentive: \", my_dataset_v3[key]['long_term_incentive']\n",
    "    #print type(my_dataset_v3[key]['long_term_incentive'])\n",
    "    #print \"payments: \", my_dataset_v3[key]['total_payments']\n",
    "    #print type(my_dataset_v3[key]['total_payments'])\n",
    "    if my_dataset_v3[key]['long_term_incentive'] != 'NaN' and my_dataset_v3[key]['total_payments'] != 'NaN':\n",
    "        if my_dataset_v3[key]['long_term_incentive'] !=0 and my_dataset_v3[key]['total_payments'] != 0:\n",
    "            \n",
    "            incentive_total_ratio = float(my_dataset_v3[key]['long_term_incentive']) / float(my_dataset_v3[key]['total_payments'])\n",
    "            my_dataset_v3[key]['incentive_total_ratio'] = incentive_total_ratio\n",
    "            \n",
    "        else:\n",
    "            my_dataset_v3[key]['incentive_total_ratio'] = 'NaN'\n",
    "               \n",
    "    else:\n",
    "        my_dataset_v3[key]['incentive_total_ratio'] = 'NaN'\n",
    "        \n",
    "       \n",
    "    #print my_dataset_v3[key]['incentive_total_ratio']\n",
    "    #print type(my_dataset_v3[key]['incentive_total_ratio']) \n",
    "\n",
    "#print my_dataset_v3['METTS MARK']\n",
    "print \"No. of features in dataset: \", len(my_dataset_v3['METTS MARK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features in dataset:  31\n"
     ]
    }
   ],
   "source": [
    "# print all existing features\n",
    "\n",
    "#for feature in features_list_all:\n",
    "    #print \"\\n\" + feature\n",
    "    \n",
    "# new feature: bonus / total_payments -> bonus_total_ratio\n",
    "\n",
    "my_dataset_v4 = my_dataset_v3\n",
    "\n",
    "for key in my_dataset_v4:\n",
    "    #print key\n",
    "    if my_dataset_v4[key]['bonus'] != 'NaN' and my_dataset_v4[key]['total_payments'] != 'NaN':\n",
    "        if my_dataset_v4[key]['bonus'] != 0 and my_dataset_v4[key]['total_payments'] != 0:\n",
    "            \n",
    "            bonus_total_ratio = float(my_dataset_v4[key]['bonus']) / float(my_dataset_v4[key]['total_payments'])\n",
    "            my_dataset_v4[key]['bonus_total_ratio'] = bonus_total_ratio\n",
    "            #print math.isinf(bonus_total_ratio)\n",
    "            \n",
    "        else:\n",
    "            my_dataset_v4[key]['bonus_total_ratio'] = 'NaN'\n",
    "           \n",
    "    else:\n",
    "        my_dataset_v4[key]['bonus_total_ratio'] = 'NaN'\n",
    "                \n",
    "\n",
    "#print my_dataset_v4['COLWELL WESLEY']\n",
    "print \"No. of features in dataset: \", len(my_dataset_v4['COLWELL WESLEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features in dataset:  32\n"
     ]
    }
   ],
   "source": [
    "# print all existing features\n",
    "\n",
    "#for feature in features_list_all:\n",
    "    #print \"\\n\" + feature\n",
    "    \n",
    "# new feature: bonus / salary -> bonus_salary_ratio\n",
    "\n",
    "my_dataset_v5 = my_dataset_v4\n",
    "\n",
    "for key in my_dataset_v5:\n",
    "    #print key\n",
    "    if my_dataset_v5[key]['bonus'] != 'NaN' and my_dataset_v5[key]['salary'] != 'NaN':\n",
    "        if my_dataset_v5[key]['bonus'] != 0 and my_dataset_v5[key]['salary'] != 0:\n",
    "            \n",
    "            bonus_salary_ratio = float(my_dataset_v5[key]['bonus']) / float(my_dataset_v5[key]['salary'])\n",
    "            my_dataset_v5[key]['bonus_salary_ratio'] = bonus_salary_ratio\n",
    "            #print math.isinf(bonus_salary_ratio)\n",
    "            \n",
    "        else:\n",
    "            my_dataset_v5[key]['bonus_salary_ratio'] = 'NaN'\n",
    "           \n",
    "    else:\n",
    "        my_dataset_v5[key]['bonus_salary_ratio'] = 'NaN'\n",
    "\n",
    "\n",
    "#print my_dataset_v5['COLWELL WESLEY']\n",
    "print \"No. of features in dataset: \", len(my_dataset_v5['COLWELL WESLEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify features with a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature, identify the number of people, where this feature has a value of \"NaN\"\n",
    "\n",
    "my_dataset_clean_2 = my_dataset_v5\n",
    "\n",
    "total_features = []\n",
    "\n",
    "for m in my_dataset_clean['METTS MARK']:\n",
    "    total_features.append(m)\n",
    "\n",
    "thresh = 0.75\n",
    "\n",
    "# create a dict where each feature is a key\n",
    "\n",
    "feature_dict = {}\n",
    "    \n",
    "for person in my_dataset_clean_2:\n",
    "    for feat in my_dataset_clean_2[person]:\n",
    "        feature_dict[feat] = {'NaNs': 0}\n",
    "        \n",
    "        \n",
    "\n",
    "for person in my_dataset_clean_2:\n",
    "    for feat in my_dataset_clean_2[person]:\n",
    "        \n",
    "        if my_dataset_clean_2[person][feat] == \"NaN\" or my_dataset_clean_2[person][feat] == np.nan:\n",
    "            #print feature_dict[feat]['NaNs']\n",
    "            #count += 1\n",
    "            #print \"Count2: \", count\n",
    "            feature_dict[feat]['NaNs'] += 1\n",
    "\n",
    "#print feature_dict        \n",
    "\n",
    "# feature_dict holds the name of the feature and the number of times this feature has a value of \"NaN\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature:  to_messages\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  deferral_payments\n",
      "NaN ratio:  0.696721311475\n",
      "\n",
      "Feature:  expenses\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  poi\n",
      "NaN ratio:  0.0\n",
      "\n",
      "Feature:  deferred_income\n",
      "NaN ratio:  0.672131147541\n",
      "\n",
      "Feature:  bonus_salary_ratio\n",
      "NaN ratio:  0.33606557377\n",
      "\n",
      "Feature:  from_poi_to_this_person\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  from_message_ratio\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  salary_log\n",
      "NaN ratio:  0.229508196721\n",
      "\n",
      "Feature:  email_address\n",
      "NaN ratio:  0.114754098361\n",
      "\n",
      "Feature:  restricted_stock_deferred\n",
      "NaN ratio:  0.893442622951\n",
      "\n",
      "Feature:  messages_total\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  shared_receipt_with_poi\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  loan_advances\n",
      "NaN ratio:  0.975409836066\n",
      "\n",
      "Feature:  from_messages\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  other\n",
      "NaN ratio:  0.262295081967\n",
      "\n",
      "Feature:  to_poi_ratio\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  to_message_ratio\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  director_fees\n",
      "NaN ratio:  0.950819672131\n",
      "\n",
      "Feature:  bonus\n",
      "NaN ratio:  0.33606557377\n",
      "\n",
      "Feature:  total_stock_value\n",
      "NaN ratio:  0.0573770491803\n",
      "\n",
      "Feature:  from_this_person_to_poi\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  total_payments_log\n",
      "NaN ratio:  0.0901639344262\n",
      "\n",
      "Feature:  from_poi_ratio\n",
      "NaN ratio:  0.295081967213\n",
      "\n",
      "Feature:  restricted_stock\n",
      "NaN ratio:  0.155737704918\n",
      "\n",
      "Feature:  long_term_incentive_log\n",
      "NaN ratio:  0.467213114754\n",
      "\n",
      "Feature:  salary\n",
      "NaN ratio:  0.229508196721\n",
      "\n",
      "Feature:  incentive_total_ratio\n",
      "NaN ratio:  0.467213114754\n",
      "\n",
      "Feature:  total_payments\n",
      "NaN ratio:  0.0901639344262\n",
      "\n",
      "Feature:  long_term_incentive\n",
      "NaN ratio:  0.467213114754\n",
      "\n",
      "Feature:  exercised_stock_options\n",
      "NaN ratio:  0.245901639344\n",
      "\n",
      "Feature:  bonus_total_ratio\n",
      "NaN ratio:  0.33606557377\n"
     ]
    }
   ],
   "source": [
    "# add ratio of NaNs for each feature\n",
    "\n",
    "dataset = my_dataset_v5\n",
    "\n",
    "#print len(my_dataset_v5)\n",
    "\n",
    "for feature in feature_dict:\n",
    "    feature_dict[feature]['NaN_ratio'] = float(feature_dict[feature]['NaNs'])/float(len(dataset))\n",
    "    \n",
    "#print feature_dict\n",
    "\n",
    "for feature in feature_dict:\n",
    "    print \"\\nFeature: \", feature\n",
    "    print \"NaN ratio: \", feature_dict[feature]['NaN_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "my_dataset_t = my_dataset_v5\n",
    "\n",
    "features_list_all = ['poi', 'to_messages', 'deferral_payments', 'expenses', 'deferred_income',  \n",
    "                     'long_term_incentive', 'from_message_ratio', 'salary_log', 'restricted_stock_deferred', \n",
    "                     'messages_total', 'shared_receipt_with_poi', 'loan_advances', 'from_messages', 'other', \n",
    "                     'to_poi_ratio', 'to_message_ratio', 'director_fees', 'bonus', 'total_stock_value', \n",
    "                     'from_poi_to_this_person', 'from_this_person_to_poi', 'total_payments_log', \n",
    "                     'from_poi_ratio', 'restricted_stock', 'long_term_incentive_log', 'salary', 'total_payments', \n",
    "                     'exercised_stock_options', 'bonus_salary_ratio', 'incentive_total_ratio', 'bonus_total_ratio']\n",
    "\n",
    "features_list_selector_9 = ['poi', 'from_message_ratio', 'messages_total', 'total_stock_value', 'bonus', \n",
    "                             'total_payments_log', \n",
    "                            'long_term_incentive_log', 'bonus_total_ratio', 'exercised_stock_options', \n",
    "                            'incentive_total_ratio']\n",
    "\n",
    "features_list_selector_3c = ['poi','deferred_income', 'loan_advances', 'bonus', 'total_stock_value', 'from_poi_ratio', \n",
    "                             'salary', 'exercised_stock_options', 'bonus_salary_ratio', 'bonus_total_ratio']\n",
    "\n",
    "features_list_selector_4 = ['poi','total_stock_value', 'bonus', 'total_payments_log', 'exercised_stock_options']\n",
    "\n",
    "features_list_selector_4m = ['poi','expenses', 'shared_receipt_with_poi', 'other', 'bonus']\n",
    "\n",
    "features_list_selector_6 = ['poi','bonus', 'total_stock_value', 'from_poi_ratio', 'exercised_stock_options',\n",
    "                            'bonus_salary_ratio', 'bonus_total_ratio']\n",
    "\n",
    "features_list_selector_6m = ['poi','deferral_payments', 'expenses', 'shared_receipt_with_poi', 'other', 'bonus',\n",
    "                             'from_poi_ratio']\n",
    "\n",
    "\n",
    "features_list_selector_9m = ['poi','deferral_payments', 'expenses', 'restricted_stock_deferred', 'shared_receipt_with_poi',\n",
    "                             'other', 'to_poi_ratio', 'bonus', 'from_poi_ratio', 'long_term_incentive_log']\n",
    "\n",
    "features_list_selector_12 = ['poi','deferred_income', 'shared_receipt_with_poi', 'loan_advances', 'bonus', \n",
    "                             'total_stock_value', 'from_poi_ratio', 'salary', 'total_payments', \n",
    "                             'exercised_stock_options', 'bonus_salary_ratio', 'incentive_total_ratio', 'bonus_total_ratio']\n",
    "\n",
    "features_list_selector_12m = ['poi','deferral_payments', 'expenses', 'restricted_stock_deferred', 'shared_receipt_with_poi',\n",
    "                              'other', 'to_poi_ratio', 'director_fees', 'bonus', 'total_stock_value', 'from_poi_ratio',\n",
    "                              'restricted_stock', 'bonus_salary_ratio']\n",
    "\n",
    "\n",
    "features_list_selector_15 = ['poi','expenses', 'deferred_income', 'long_term_incentive', 'salary_log', \n",
    "                             'shared_receipt_with_poi', 'loan_advances', 'bonus', 'total_stock_value', \n",
    "                             'from_poi_ratio', 'salary', 'total_payments', 'exercised_stock_options', \n",
    "                             'bonus_salary_ratio', 'incentive_total_ratio', 'bonus_total_ratio']\n",
    "\n",
    "features_list_selector_15m = ['poi','expenses', 'deferred_income', 'shared_receipt_with_poi', 'other', 'to_poi_ratio', \n",
    "                              'bonus', 'total_stock_value', 'from_this_person_to_poi', 'from_poi_ratio', \n",
    "                              'restricted_stock', 'total_payments', 'exercised_stock_options', 'bonus_salary_ratio',\n",
    "                              'incentive_total_ratio', 'bonus_total_ratio']\n",
    "\n",
    "\n",
    "\n",
    "data_dt = featureFormat(my_dataset_t, features_list_selector_9, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data_dt)\n",
    "\n",
    "\n",
    "# Split into training and test data\n",
    "\n",
    "folds = 1000\n",
    "cv = StratifiedShuffleSplit(labels, folds, random_state = 25,\n",
    "        test_size = 0.15)\n",
    "\n",
    "for train_idx, test_idx in cv:\n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectKBest features from all features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select K-best features\n",
    "\n",
    "# list of all (current) features\n",
    "\n",
    "my_features = []\n",
    "\n",
    "for k in my_dataset_v5['METTS MARK']:\n",
    "    my_features.append(k)\n",
    "\n",
    "#print my_features\n",
    "#print len(my_features)\n",
    "\n",
    "# important: 'poi' has to be the first feature!! \n",
    "# remove email adress as this feature is a string and causes problems in the featureFormat fucntion\n",
    "features_list_all = ['poi', 'to_messages', 'deferral_payments', 'expenses', 'deferred_income',  \n",
    "                     'long_term_incentive', 'from_message_ratio', 'salary_log', 'restricted_stock_deferred', \n",
    "                     'messages_total', 'shared_receipt_with_poi', 'loan_advances', 'from_messages', 'other', \n",
    "                     'to_poi_ratio', 'to_message_ratio', 'director_fees', 'bonus', 'total_stock_value', \n",
    "                     'from_poi_to_this_person', 'from_this_person_to_poi', 'total_payments_log', \n",
    "                     'from_poi_ratio', 'restricted_stock', 'long_term_incentive_log', 'salary', 'total_payments', \n",
    "                     'exercised_stock_options', 'bonus_salary_ratio', 'incentive_total_ratio', 'bonus_total_ratio']\n",
    "\n",
    "\n",
    "#print len(features_list_all)\n",
    "# feature selection\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# create the selector\n",
    "\n",
    "selector = SelectKBest(f_classif, k = 9)\n",
    "\n",
    "selector.fit(features_train, labels_train)\n",
    "\n",
    "#print \"\\n\", selector.scores_\n",
    "#print len(selector.scores_)\n",
    "\n",
    "# get the selected k best features by first getting their indices calling .get_support\n",
    "# get the features from list with all features by selecting their index \n",
    "indexes = selector.get_support(indices=True)\n",
    "\n",
    "#print indexes\n",
    "\n",
    "\n",
    "k_features = [features_list_all[i+1] for i in indexes]\n",
    "\n",
    "print \"\\nK-Best features: \", k_features\n",
    "\n",
    "\n",
    "#print \"features_train: \", features_train[0]\n",
    "\n",
    "# reduce features_train to the k best features by transforming\n",
    "features_train_transformed = selector.transform(features_train)\n",
    "\n",
    "#print \"\\nfeatures_train_transformed: \", features_train_transformed[0]\n",
    "\n",
    "# reduce features_test to the k best features by transforming\n",
    "features_test_transformed = selector.transform(features_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0.001 s\n",
      "Prediction time: 0.002 s\n",
      "Accuracy:  0.8421052631578947\n",
      "No. of features:  9\n",
      "feature weight:  [0.         0.         0.35866571 0.37107195 0.21631066 0.\n",
      " 0.         0.05395168 0.        ]\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split=11)\n",
    "# try different parameters\n",
    "clf_dt_1 = tree.DecisionTreeClassifier(criterion='gini', min_samples_split=11, splitter='best')\n",
    "\n",
    "t0 = time()\n",
    "clf_dt_1.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\" # taking time part 2\n",
    "\n",
    "t1 = time() # timing the prediction\n",
    "pred = clf_dt_1.predict(features_test)\n",
    "print \"Prediction time:\", round(time()-t0, 3), \"s\" # prediction time part 2\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(pred, labels_test) \n",
    "\n",
    "print \"Accuracy: \", acc\n",
    "\n",
    "no_features = len(features_train[0])\n",
    "print \"No. of features: \", no_features\n",
    "\n",
    "\n",
    "features_weight = clf_dt_1.feature_importances_\n",
    "\n",
    "print \"feature weight: \", features_weight\n",
    "\n",
    "#print \"\\ntester result: \", test_classifier(clf_dt_1, my_dataset_t, features_list_selector_3c, folds=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tester result:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=11,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.82400\tPrecision: 0.41252\tRecall: 0.33950\tF1: 0.37246\tF2: 0.35196\n",
      "\tTotal predictions: 13000\tTrue positives:  679\tFalse positives:  967\tFalse negatives: 1321\tTrue negatives: 10033\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print \"\\ntester result: \", test_classifier(clf_dt_1, my_dataset_t, features_list_selector_9, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with GridsearchCV\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "#features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'min_samples_split': [2,3,5,7,9,11,13,15,17,19],\n",
    "             'max_features': [2,3,4,5,6,7,8,9] }\n",
    "\n",
    "parameters_2 = {'min_samples_split': [2,3,5,7,9,11,13,15,17,19]}\n",
    "\n",
    "parameters_3 = {'max_features': [2,3,4,5,6,7,8,9]}\n",
    "\n",
    "# pass scoring parameters by which gridsearch evaluates the performance\n",
    "scoring = {'prec': 'precision', 'rec': 'recall'}\n",
    "\n",
    "tree = tree.DecisionTreeClassifier()\n",
    "\n",
    "clf = GridSearchCV(tree, parameters_3, scoring = scoring, refit = 'prec')\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\" # taking time part 2\n",
    "\n",
    "t1 = time() # timing the prediction\n",
    "pred = clf.predict(features_test)\n",
    "print \"Prediction time:\", round(time()-t0, 3), \"s\" # prediction time part 2\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(pred, labels_test) \n",
    "\n",
    "print \"Accuracy: \", acc\n",
    "\n",
    "no_features = len(features_train[0])\n",
    "print \"No. of features: \", no_features\n",
    "\n",
    "best_params = clf.best_params_\n",
    "print best_params\n",
    "\n",
    "print \"\\ntester result: \", test_classifier(clf, my_dataset_t, features_list_selector_9, folds=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune classifier and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# get stats (min, max, mean, median) of each feature (column)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features_train_info = features_train\n",
    "\n",
    "#for a in features_info:\n",
    "    #print a\n",
    "\n",
    "def feature_stats(features):\n",
    "    # turn features (list of arrays) into an array\n",
    "    \n",
    "    feature_array = np.empty((0,len(features[0])), float) # create empty array with the correct dimensions\n",
    "    \n",
    "    #print feature_array.shape\n",
    "    \n",
    "    for i in range(0, len(features)):\n",
    "        #print features[i].shape\n",
    "        # reshape 1D array into 2D array with 1 row and No. of features columns\n",
    "        reshape = np.reshape(features[i], (1,len(features[0]))) \n",
    "        \n",
    "        feature_array = np.vstack([feature_array, reshape])\n",
    "        \n",
    "        #np.append(feature_array, reshape, axis=0)\n",
    "    \n",
    "    #print feature_array\n",
    "    \n",
    "    # create empty dict of dict for storing min, max, mean, median per feature (column)\n",
    "    \n",
    "    #print \"\\n\", feature_array[:,1]\n",
    "    \n",
    "    stat_dict = {}\n",
    "    \n",
    "    for x in range (0, len(feature_array[0,:])):\n",
    "        #print len(feature_array[0,:])\n",
    "        temp_dict = {}\n",
    "        # use nanmax to ignore NaNs in the columns\n",
    "        temp_dict[\"max\"] = np.nanmax(feature_array[:,x])\n",
    "        temp_dict[\"min\"] = np.nanmin(feature_array[:,x])\n",
    "        temp_dict[\"mean\"] = np.nanmean(feature_array[:,x])\n",
    "        temp_dict[\"median\"] = np.nanmedian(feature_array[:,x])\n",
    "        \n",
    "        stat_dict[x] = temp_dict\n",
    "        \n",
    "        plt.hist(feature_array[:,x], bins = 'auto', range = (np.nanmin(feature_array[:,x]), np.nanmax(feature_array[:,x]) ))\n",
    "        plt.title(\"Histogram of feature {} with 10 bins\".format(x))\n",
    "        plt.show()\n",
    "        #print \"\\nFeature \", features_list[x+1]\n",
    "        #print \"Max: \", rescaled_max\n",
    "        #print \"Min: \", rescaled_min\n",
    "        #print \"Average: \", rescaled_mean\n",
    "        #print \"Median: \", rescaled_median\n",
    "    \n",
    "    # returns a dict of dict. The feature number (index) is the key to a dict containing the max, min, mean\n",
    "    # and median (keys) values for this feature\n",
    "    return stat_dict\n",
    "    \n",
    "print feature_stats(features_train_info)\n",
    "#print \"\\n\", feature_stats(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For features where the mode of the distribution or a very high peak in the histogram is at 0 -> impute 0s in this feature with the mean (as the median is also 0 in some cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute 0 values in array for certain features with median or mean\n",
    "# use scikit learn preprocessing Imputer \n",
    "# -> http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values\n",
    "\n",
    "# only use for features where it makes sense e.g salaray, total payments...\n",
    "# first replace 0 in columns of interest with NaN for NaN to be replaced by the Imputer\n",
    "\n",
    "# The featureFormat function replaces \"NaN\" in the dataset with 0 -> so the 0s in features_train etc have probably been \"NaN\"\n",
    "# in the original dataset, but there is no way to be sure. Does transforming them back to \"NaN\" and then\n",
    "# imputing them make sense?\n",
    "\n",
    "\n",
    "\n",
    "#print len(features_train)\n",
    "\n",
    "features_train_nan = features_train\n",
    "\n",
    "#print \"Features before: \", features_train_nan\n",
    "\n",
    "features_index = [1,3,4,5,6,7,8]\n",
    "\n",
    "col_array = np.array([[]])\n",
    "\n",
    "\n",
    "def nan_in_features_of_interest(features, feature_index_list):\n",
    "    # replaces 0 in features (list of arrays, e.g. features_train) in columns of interest \n",
    "    # (index in array given by feature_index_list) with \"NaN\" -> preparation for Imputer\n",
    "    array_out = []\n",
    "    for b in range(0, len(features)):\n",
    "        #print features[b]\n",
    "        array = features[b].copy()\n",
    "        for feat in feature_index_list:\n",
    "            #print features[b][feat]\n",
    "            if array[feat] == 0.:\n",
    "                array[feat] = np.nan\n",
    "        array_out.append(array)\n",
    "    return array_out\n",
    "                           \n",
    "\n",
    "#print \"\\n\", features_nan \n",
    "\n",
    "#print \"\\n\", nan_in_features_of_interest(features_train, features_index)\n",
    "\n",
    "features_train_nan = nan_in_features_of_interest(features_train_nan, features_index)\n",
    "\n",
    "print \"\\nFeatures with NaN: \", features_train_nan\n",
    "# Impute NaN with median value for that column\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute - replace NaN in columns of interest with median of that column\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer_median = Imputer(missing_values='NaN', strategy='median', axis=0, copy=True)\n",
    "\n",
    "features_train_impute_median = imputer_median.fit_transform(features_nan)\n",
    "\n",
    "#print features_impute_median\n",
    "\n",
    "imputer_mean = Imputer(missing_values='NaN', strategy='mean', axis=0, copy=True)\n",
    "\n",
    "features_train_impute_mean = imputer_mean.fit_transform(features_nan)\n",
    "\n",
    "print features_impute_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute function -> replace 'NaN' with the mean of that feature\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "missing_values = 'NaN'\n",
    "strategy = 'mean'\n",
    "axis = 0\n",
    "\n",
    "def imputer_mean(features_in):\n",
    "    imputer = Imputer(missing_values='NaN', strategy='mean', axis=0, copy=True)\n",
    "    features_out = imputer.fit_transform(features_in)\n",
    "    return features_out\n",
    "\n",
    "#features_train_impute_mean = imputer_mean(features_train_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DT classifier again with imputed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "\n",
    "features_dt_train = features_train    \n",
    "labels_dt_train = labels_train   \n",
    "features_dt_test = features_test   \n",
    "labels_dt_test = labels_test  \n",
    "\n",
    "\n",
    "# replace 0 in features where most of the values are 0 (see features_stat function) with 'NaN'\n",
    "features_index = [1,3,4,5,6,7,8]\n",
    "features_train_nan = nan_in_features_of_interest(features_dt_train, features_index)\n",
    "features_test_nan = nan_in_features_of_interest(features_dt_test, features_index)\n",
    "\n",
    "# impute 'NaN' in features (see above) with the mean value of that feature\n",
    "features_train_impute_mean = imputer_mean(features_train_nan)\n",
    "features_test_impute_mean = imputer_mean(features_test_nan)\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import tree\n",
    "clf_imp = tree.DecisionTreeClassifier(min_samples_split=10)\n",
    "\n",
    "t0 = time()\n",
    "clf_imp.fit(features_train_impute_mean, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\" # taking time part 2\n",
    "\n",
    "t1 = time() # timing the prediction\n",
    "pred = clf_imp.predict(features_test_impute_mean)\n",
    "print \"Prediction time:\", round(time()-t0, 3), \"s\" # prediction time part 2\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(pred, labels_test) \n",
    "\n",
    "print \"Accuracy: \", acc\n",
    "\n",
    "no_features = len(features_train_impute_mean[0])\n",
    "print \"No. of features: \", no_features\n",
    "\n",
    "\n",
    "features_weight = clf_imp.feature_importances_\n",
    "\n",
    "print \"feature weight: \", features_weight\n",
    "\n",
    "#tree.export_graphviz(clf, out_file='tree.dot') \n",
    "\n",
    "print \"\\ntester result: \", test_classifier(clf_imp, my_dataset_t, features_list_selector_9, folds=1000)\n",
    "\n",
    "# impute seems to have a small negative effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print features_test\n",
    "\n",
    "feature_dict = {}\n",
    "\n",
    "# features_test is a list of numpy arrays, the len of one array is the number of features used\n",
    "feat_len = len(features_test[0])\n",
    "\n",
    "# create an empty list for each feature in the empty feature dictionary\n",
    "for a in range(0, feat_len):\n",
    "    feature_dict[a] = []\n",
    "\n",
    "# append all the values to the corresponding feature list\n",
    "for feature in features_test:\n",
    "    for i in range(0, len(feature)):\n",
    "        feature_dict[i].append(feature[i])\n",
    "        \n",
    "# get the min/max, average values for each of the features\n",
    "for key in feature_dict:\n",
    "    print \"\\nFeature \", features_list[key+1]\n",
    "    print \"max: \", max(feature_dict[key])\n",
    "    print \"min: \", min(feature_dict[key])\n",
    "    print \"average: \", sum(feature_dict[key])/len(feature_dict[key])\n",
    "    print \"median: \", np.median(feature_dict[key])\n",
    "#print feature_dict\n",
    "#print len(feature_dict[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_features(features):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # first put list of values from feature_dict into list of list\n",
    "    # reshape list of list into numpy array ---- or use features_test (which is a list of np arrays)\n",
    "\n",
    "    rescaled = scaler.fit_transform(features)\n",
    "    \n",
    "    return rescaled\n",
    "\n",
    "rescaled = scale_features(features_test)\n",
    "#print rescaled\n",
    "\n",
    "#print len(rescaled[0,:])\n",
    "\n",
    "# get min, max, average for each of the rescaled columns\n",
    "\n",
    "c_index = 0\n",
    "\n",
    "for x in range (0, len(rescaled[0,:])):\n",
    "    rescaled_max = np.max(rescaled[:,x])\n",
    "    rescaled_min = np.min(rescaled[:,x])\n",
    "    rescaled_mean = np.mean(rescaled[:,x])\n",
    "    rescaled_median = np.median(rescaled[:,x])\n",
    "    hist = np.histogram(rescaled[:,x])\n",
    "    \n",
    "    print \"\\nFeature \", features_list[x+1]\n",
    "    print \"Max: \", rescaled_max\n",
    "    print \"Min: \", rescaled_min\n",
    "    print \"Average: \", rescaled_mean\n",
    "    print \"Median: \", rescaled_median\n",
    "    print \"\\n\"\n",
    "    plt.hist(rescaled[:,x], bins = 10)\n",
    "    plt.title(\"Histogram of feature with 10 bins\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "\n",
    "# replace 0 in features where most of the values are 0 (see features_stat function) with 'NaN'\n",
    "features_index = [1,3,4,5,6,7,8]\n",
    "features_train_nan_n = nan_in_features_of_interest(features_train, features_index)\n",
    "features_test_nan_n = nan_in_features_of_interest(features_test, features_index)\n",
    "\n",
    "# impute 'NaN' in features (see above) with the mean value of that feature\n",
    "features_train_impute_mean_n = imputer_mean(features_train_nan_n)\n",
    "features_test_impute_mean_n = imputer_mean(features_test_nan_n)\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_nb = GaussianNB()\n",
    "\n",
    "\n",
    "# rescale features\n",
    "\n",
    "#scaled_train = scale_features(features_nb_train)\n",
    "#scaled_test = scale_features(features_nb_test)\n",
    "\n",
    "t0 = time() # use checking how long fitting the classifier takes\n",
    "clf_nb.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\" # taking time part 2\n",
    "\n",
    "t1 = time() # timing the prediction\n",
    "pred = clf_nb.predict(features_test)\n",
    "print \"Prediction time:\", round(time()-t0, 3), \"s\" # prediction time part 2\n",
    "\n",
    "#print pred\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(pred, labels_test) # accuracy goes down from 98,4% to 88,4% when slicing training set to 1%\n",
    "\n",
    "print acc\n",
    "\n",
    "print \"\\ntester result: \", test_classifier(clf_nb, my_dataset_t, features_list_selector_9, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA + Naive Bayes or Decision Tree using a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with PCA and classifier (Naive Bayes or Decision tree)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Classifier\n",
    "clf_dt_p = tree.DecisionTreeClassifier(min_samples_split=10)\n",
    "\n",
    "clf_NB_p = GaussianNB()\n",
    "\n",
    "estimators = [('PCA', pca), ('clf', clf_dt_p)]\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(features_train, labels_train)\n",
    "\n",
    "pred = pipe.predict(features_test)\n",
    "\n",
    "\n",
    "print \"\\nPCA - explained variance: \", pca.explained_variance_ratio_\n",
    "\n",
    "first_pc = pca.components_[0]\n",
    "\n",
    "#print \"\\nFirst PC: \", first_pc\n",
    "\n",
    "print \"\\ntester result: \", test_classifier(pipe, my_dataset_t, features_list_all, folds=1000)\n",
    "\n",
    "# Best settings for NB: PCA - n_components = 8\n",
    "# Best settings for DT: min_samples_split = 10; PCA - n_components = 3\n",
    "#           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump decision tree classifier\n",
    "\n",
    "clf_sub = clf_dt_1\n",
    "my_dataset_sub = my_dataset_v5\n",
    "features_list_sub = features_list_selector_9\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf_sub, my_dataset_sub, features_list_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_sub2 = pipe\n",
    "my_dataset_sub2 = my_dataset_v5\n",
    "features_list_sub2 = features_list_all\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf_sub2, my_dataset_sub2, features_list_sub2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://machinelearningmastery.com/handle-missing-data-python/\n",
    "http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values\n",
    "https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/\n",
    "https://stackoverflow.com/questions/22392497/how-to-add-a-new-row-to-an-empty-numpy-array/22394181\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.nanmax.html\n",
    "https://matplotlib.org/gallery/statistics/histogram_features.html\n",
    "https://stackoverflow.com/questions/3179106/python-select-subset-from-list-based-on-index-set\n",
    "https://stats.stackexchange.com/questions/244507/what-algorithms-need-feature-scaling-beside-from-svm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
